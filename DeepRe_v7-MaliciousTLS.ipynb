{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f90a0f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.cluster import KMeans\n",
    "import csv\n",
    "\n",
    "from gcxyuan.LNL import Data_Process, wave_tran_4\n",
    "from gcxyuan.model import MLPAE_for_DeepRe\n",
    "from gcxyuan.plot import TSNE_plot\n",
    "from gcxyuan.model_MoCo import MoCo\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54e9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(algorithm = 'MCRe',\n",
    "              dataset = 'Malicious_TLS',\n",
    "              #data = \"D:\\\\Jupyter\\\\UCL\\\\data\\\\CICIDS2017.csv\",\n",
    "              data = \"G:\\\\tls_features\\\\malicious_TLS_4_paper\\\\label_encodered_malicious_TLS.csv\",\n",
    "              #data = \"D:\\\\Jupyter\\\\UCL\\\\data\\\\iot23--1444674--12classes.csv\",\n",
    "              \n",
    "              savedir = './results',\n",
    "              noise_pattern = 'sym', ##asym or sym\n",
    "              INCV_C_list = [0.6],#[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "              percent = 0.7,\n",
    "              #seed = 1,\n",
    "              \n",
    "              batch_size = 256, \n",
    "              num_workers =1,\n",
    "              epochs = 200,\n",
    "              adjust_lr = 1,\n",
    "              learning_rate = 1e-2,\n",
    "              \n",
    "              embedding_size = 128,\n",
    "              #start_clean_epoch = 100,\n",
    "              #epoch_contrast = 50, \n",
    "              \n",
    "              moco_queue = 8192,\n",
    "              moco_m = 0.999,\n",
    "              temperature = 0.1,\n",
    "              alpha = 0.5,\n",
    "              pseudo_th = 0.8,\n",
    "              proto_m = 0.999,\n",
    "              lr = 0.05,\n",
    "              cos = False,\n",
    "              schedule = [40, 80],\n",
    "              w_proto = 1,\n",
    "              w_inst = 1,\n",
    "              print_freq = 300,\n",
    "              \n",
    "                          \n",
    "              \n",
    "              num_class = 23, #\n",
    "              low_dim = 16,\n",
    "              train_size = 0,\n",
    "              val_size = 0,\n",
    "              input_dim = 117,\n",
    "              \n",
    "              \n",
    "              \n",
    "              ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "487d0e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, config):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    lr = config['lr']\n",
    "    if config['cos']:  # cosine lr schedule\n",
    "        lr *= 0.5 * (1. + math.cos(math.pi * epoch / config['epochs']))\n",
    "    else:  # stepwise lr schedule\n",
    "        for milestone in config['schedule']:\n",
    "            lr *= 0.1 if epoch >= milestone else 1.\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'    \n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res  \n",
    "\n",
    "def acc(y_true, y_pred, num_cluster):\n",
    "\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    y_pred = y_pred.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "\n",
    "    w = np.zeros((num_cluster, num_cluster))\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    \n",
    "    ind = linear_sum_assignment(w.max() - w)\n",
    "    accuracy = 0.0\n",
    "    for i in ind[0]:\n",
    "        accuracy = accuracy + w[i, ind[1][i]]\n",
    "    return accuracy / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c829a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Kmeans_model_evaluation_Discrete(model, dataloader, dataset = 'train'):\n",
    "    \n",
    "    if dataset == 'train':\n",
    "        datasize = config['train_size']\n",
    "    elif dataset == 'val':\n",
    "        datasize = config['val_size']\n",
    "        \n",
    "    model.eval()\n",
    "    datas = np.zeros([datasize, config['embedding_size']])\n",
    "    label_true = np.zeros(datasize)\n",
    "    ii = 0\n",
    "    for i, (x, target, indexes) in enumerate(dataloader):\n",
    "        x = x.reshape(config['batch_size'],1,-1)\n",
    "        x = Variable(x).cuda()\n",
    "        target = Variable(target).cuda()\n",
    "        \n",
    "        _, u = model(x)\n",
    "        u = u.cpu()\n",
    "        datas[ii * config['batch_size']:(ii + 1) * config['batch_size'], :] = u.data.numpy()\n",
    "        label_true[ii * config['batch_size']:(ii + 1) * config['batch_size']] = target.cpu().numpy().reshape((1,-1))\n",
    "        ii = ii + 1\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=config['num_classes'], random_state=0).fit(datas)\n",
    "    centers = kmeans.cluster_centers_##\n",
    "    print(len(kmeans.labels_))\n",
    "    print(kmeans.labels_)\n",
    "    \n",
    "    label_pred = kmeans.labels_\n",
    "    print(label_true)\n",
    "    ACC = acc(label_true, label_pred, config['num_classes'])\n",
    "    return ACC, centers\n",
    "\n",
    "def Kmeans_model_evaluation(model, train_dataloader, val_dataloader):\n",
    "    \n",
    "    train_datasize = config['train_size']\n",
    "    val_datasize = config['val_size']\n",
    "        \n",
    "    #model.eval()\n",
    "    ##train_data\n",
    "    train_datas = np.zeros([train_datasize, 32])#config['embedding_size']\n",
    "    train_label_observed = np.zeros(train_datasize)\n",
    "    ii = 0\n",
    "    for i, (x, target, indexes) in enumerate(train_dataloader):\n",
    "        x = x.reshape(config['batch_size'],1,-1)\n",
    "        x = Variable(x).cuda()\n",
    "        target = Variable(target).cuda()\n",
    "        \n",
    "        _,_,_,_,_, u = model(x,target,config)\n",
    "        u = u.cpu()\n",
    "        train_datas[ii * config['batch_size']:(ii + 1) * config['batch_size'], :] = u.data.numpy()\n",
    "        train_label_observed[ii * config['batch_size']:(ii + 1) * config['batch_size']] = target.cpu().numpy().reshape((1,-1))\n",
    "        ii = ii + 1\n",
    "    \n",
    "    ##val_data\n",
    "    val_datas = np.zeros([val_datasize, 32])#config['embedding_size']\n",
    "    val_label_true = np.zeros(val_datasize)\n",
    "    ii = 0\n",
    "    for i, (x, target, indexes) in enumerate(val_dataloader):\n",
    "        x = x.reshape(config['batch_size'],1,-1)\n",
    "        x = Variable(x).cuda()\n",
    "        target = Variable(target).cuda()\n",
    "        \n",
    "        _,_,_,_,_, u = model(x,target,config)\n",
    "        u = u.cpu()\n",
    "        val_datas[ii * config['batch_size']:(ii + 1) * config['batch_size'], :] = u.data.numpy()\n",
    "        val_label_true[ii * config['batch_size']:(ii + 1) * config['batch_size']] = target.cpu().numpy().reshape((1,-1))\n",
    "        ii = ii + 1\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=config['num_classes'], random_state=0).fit(train_datas)\n",
    "    centers = kmeans.cluster_centers_##\n",
    "    \n",
    "    train_label_pred = kmeans.labels_\n",
    "    \n",
    "    train_ACC = acc(train_label_observed, train_label_pred, config['num_classes'])\n",
    "    print('train acc:',train_ACC)\n",
    "    \n",
    "    val_label_pred = kmeans.predict(val_datas)\n",
    "    #kmeans_val = KMeans(n_clusters=config['num_classes'], random_state=0).fit(val_datas)\n",
    "    #val_label_pred = kmeans_val.labels_\n",
    "    val_ACC = acc(val_label_true, val_label_pred, config['num_classes'])\n",
    "    print('val acc:',val_ACC)\n",
    "    \n",
    "    return train_ACC, val_ACC, centers\n",
    "\n",
    "\n",
    "def clusterloss(config, u, rho = 1.0, reduction='none'):\n",
    "              \n",
    "    kmeans = KMeans(n_clusters=config['num_classes'], random_state=0).fit(u.cpu().detach())\n",
    "    loss = silhouette_score(u.cpu().detach(),kmeans.labels_)\n",
    "    \n",
    "    return loss \n",
    "    \n",
    "\n",
    "def DeepRe():\n",
    "    \n",
    "    cls_acc = []\n",
    "    kmeans_acc = []\n",
    "    \n",
    "    data = pd.read_csv(config['data'])\n",
    "    class_le = LabelEncoder()\n",
    "    data['Label'] = class_le.fit_transform(data['Label'])\n",
    "    df_train = data.sample(frac = config['percent'])  \n",
    "    df_val = data[~data.index.isin(df_train.index)]  \n",
    "    \n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_val = df_val.reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = Data_Process(data = df_train,\n",
    "                               train=True,\n",
    "                               transform = transforms.ToTensor(),\n",
    "                               noise_type = config['noise_pattern'],\n",
    "                               INCV_b = INCV_b,\n",
    "                               INCV_c = INCV_c             \n",
    "                               )\n",
    "        \n",
    "    val_dataset = Data_Process(data = df_val,#\n",
    "                               train=False,\n",
    "                               transform = transforms.ToTensor(),\n",
    "                               noise_type = config['noise_pattern'],\n",
    "                               INCV_b = INCV_b,\n",
    "                               INCV_c = INCV_c             \n",
    "                               )\n",
    "    \n",
    "    config['train_size'] = len(train_dataset)\n",
    "    config['val_size'] = len(val_dataset)\n",
    "    config['num_classes'] = len(np.unique(data['Label']))\n",
    "    config['input_dim'] = df_train.shape[1] - 1\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=config['batch_size'], \n",
    "                                               num_workers=config['num_workers'],\n",
    "                                               drop_last=True,\n",
    "                                               shuffle=True)\n",
    "        \n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                              batch_size=config['batch_size'], \n",
    "                                              num_workers=config['num_workers'],\n",
    "                                              drop_last=True,\n",
    "                                              shuffle=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################################################\n",
    "    \n",
    "    print('building model...')\n",
    "    \n",
    "    model = MoCo(MLPAE_for_DeepRe,config)\n",
    "    model.cuda()\n",
    "    \n",
    "    criterion2 = nn.CrossEntropyLoss()#.cuda()\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = config['lr'],\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4)\n",
    "    \n",
    "    for epoch in range(1,config['epochs']):\n",
    "        print('epoch:',epoch)\n",
    "           \n",
    "        adjust_learning_rate(optimizer, epoch, config)\n",
    "        \n",
    "        #train(train_loader, model, criterion, optimizer, epoch, args, logger)\n",
    "        \n",
    "        batch_time = AverageMeter('Time', ':1.2f')\n",
    "        data_time = AverageMeter('Data', ':1.2f')   \n",
    "        acc_cls = AverageMeter('Acc@Cls', ':2.2f')\n",
    "        acc_proto = AverageMeter('Acc@Proto', ':2.2f')\n",
    "        #acc_inst = AverageMeter('Acc@Inst', ':2.2f')\n",
    "        \n",
    "        progress = ProgressMeter(\n",
    "            len(train_loader),\n",
    "            [batch_time, data_time, acc_cls, acc_proto],\n",
    "            prefix=\"Epoch: [{}]\".format(epoch))\n",
    "        \n",
    "        ##开始训练\n",
    "        model.train()\n",
    "        end = time.time()\n",
    "        \n",
    "        for i, (x, target_, indexes) in enumerate(train_loader):\n",
    "            x = x.reshape(config['batch_size'],1,-1)\n",
    "            x = Variable(x).cuda()\n",
    "            target_ = Variable(target_).cuda()\n",
    "            \n",
    "            data_time.update(time.time() - end)\n",
    "            \n",
    "            loss = 0\n",
    "            \n",
    "            # compute model output               \n",
    "            cls_out, target, logits, x_q, logits_proto, u = \\\n",
    "            model(x, target_,config,is_proto=(epoch>0))\n",
    "                       \n",
    "                \n",
    "            loss_proto = criterion2(logits_proto, target.squeeze(1))\n",
    "            acc = accuracy(logits_proto, target)[0] \n",
    "            acc_proto.update(acc[0]) \n",
    "            \n",
    "            \n",
    "            loss_cls = criterion2(cls_out, target.squeeze(1)) \n",
    "            loss_clus =  clusterloss(config, u)\n",
    "            loss_clus = torch.Tensor([loss_clus]).cuda()\n",
    "            loss_AE = nn.MSELoss()(x, x_q)\n",
    "            x_1 = x.reshape(config['batch_size'],-1)\n",
    "            loss = loss_cls + config['w_proto']*loss_proto + loss_AE + loss_clus\n",
    "            \n",
    "            # log accuracy\n",
    "            acc = accuracy(cls_out, target)[0] \n",
    "            acc_cls.update(acc[0])\n",
    "               \n",
    "             \n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()#######\n",
    "            optimizer.step()\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            #if i % config['print_freq'] == 0:\n",
    "                #progress.display(i)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "            print('==> Evaluation...')       \n",
    "            model.eval()    \n",
    "            top1_acc = AverageMeter(\"Top1\")\n",
    "            top5_acc = AverageMeter(\"Top5\")\n",
    "            \n",
    "            # evaluate on webvision val set\n",
    "            for batch_idx, (x, target_, indexes) in enumerate(val_loader):\n",
    "                x = x.reshape(config['batch_size'],1,-1)\n",
    "                x = Variable(x).cuda()\n",
    "                target_ = Variable(target_).cuda()\n",
    "                                \n",
    "                outputs,_,target = model(x, target_, config, is_eval=True)    \n",
    "                acc1 = accuracy(outputs, target)\n",
    "                top1_acc.update(acc1[0])\n",
    "                            \n",
    "            # average across all processes\n",
    "            acc_tensors = torch.Tensor([top1_acc.avg]).cuda()\n",
    "                        \n",
    "            print('Accuracy is %.2f%%',acc_tensors[0].data.cpu().numpy())\n",
    "            \n",
    "            train_ACC, val_ACC, centers = Kmeans_model_evaluation(model = model, train_dataloader= train_loader, val_dataloader = val_loader)\n",
    "        \n",
    "        cls_acc.append(acc_tensors[0].data.cpu().numpy())\n",
    "        kmeans_acc.append(val_ACC)\n",
    "        \n",
    "    f = open(config['dataset']+'_'+config['noise_pattern']+'_res.csv','a',newline='')\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow(cls_acc)\n",
    "    csv_writer.writerow(kmeans_acc)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ede564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCV_c: 0.6\n",
      "building model...\n",
      "epoch: 1\n",
      "==> Evaluation...\n",
      "Accuracy is %.2f%% 37.03636\n",
      "train acc: 0.12233018750776108\n",
      "val acc: 0.24454907642158638\n",
      "epoch: 2\n",
      "==> Evaluation...\n",
      "Accuracy is %.2f%% 37.03636\n",
      "train acc: 0.22415559418850117\n",
      "val acc: 0.5352770735240855\n",
      "epoch: 3\n",
      "==> Evaluation...\n",
      "Accuracy is %.2f%% 39.051548\n",
      "train acc: 0.2900006208866261\n",
      "val acc: 0.6950018109380659\n",
      "epoch: 4\n",
      "==> Evaluation...\n",
      "Accuracy is %.2f%% 37.03636\n",
      "train acc: 0.28919346827269343\n",
      "val acc: 0.6755885548714234\n",
      "epoch: 5\n",
      "==> Evaluation...\n",
      "Accuracy is %.2f%% 42.92129\n",
      "train acc: 0.2949987582267478\n",
      "val acc: 0.7288663527707352\n",
      "epoch: 6\n",
      "==> Evaluation...\n",
      "Accuracy is %.2f%% 42.738754\n",
      "train acc: 0.2977772258785546\n",
      "val acc: 0.7372691053965954\n",
      "epoch: 7\n",
      "==> Evaluation...\n",
      "Accuracy is %.2f%% 37.222546\n",
      "train acc: 0.3097758599279771\n",
      "val acc: 0.7655197392249186\n",
      "epoch: 8\n",
      "==> Evaluation...\n",
      "Accuracy is %.2f%% 39.193924\n",
      "train acc: 0.3346734136346703\n",
      "val acc: 0.8336110105034408\n",
      "epoch: 9\n",
      "==> Evaluation...\n",
      "Accuracy is %.2f%% 50.872517\n",
      "train acc: 0.34567862908232955\n",
      "val acc: 0.83871785584933\n",
      "epoch: 10\n",
      "==> Evaluation...\n",
      "Accuracy is %.2f%% 41.09959\n",
      "train acc: 0.3457717620762449\n",
      "val acc: 0.8472292647591453\n",
      "epoch: 11\n",
      "==> Evaluation...\n",
      "Accuracy is %.2f%% 45.95502\n",
      "train acc: 0.34803799826151743\n",
      "val acc: 0.8647591452372329\n",
      "epoch: 12\n",
      "==> Evaluation...\n",
      "Accuracy is %.2f%% 47.98846\n",
      "train acc: 0.35072333291940894\n",
      "val acc: 0.87145961608113\n",
      "epoch: 13\n",
      "==> Evaluation...\n",
      "Accuracy is %.2f%% 37.5073\n",
      "train acc: 0.34980752514590835\n",
      "val acc: 0.8675117710974285\n",
      "epoch: 14\n",
      "==> Evaluation...\n",
      "Accuracy is %.2f%% 47.057533\n",
      "train acc: 0.35429343101949584\n",
      "val acc: 0.8776530242665701\n",
      "epoch: 15\n",
      "==> Evaluation...\n",
      "Accuracy is %.2f%% 43.224297\n",
      "train acc: 0.3496988699863405\n",
      "val acc: 0.8641434262948208\n",
      "epoch: 16\n",
      "==> Evaluation...\n",
      "Accuracy is %.2f%% 41.81878\n",
      "train acc: 0.3544486526760214\n",
      "val acc: 0.8797537124230351\n",
      "epoch: 17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     INCV_b \u001b[38;5;241m=\u001b[39m INCV_c\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINCV_c:\u001b[39m\u001b[38;5;124m\"\u001b[39m,INCV_c)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mDeepRe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 208\u001b[0m, in \u001b[0;36mDeepRe\u001b[1;34m()\u001b[0m\n\u001b[0;32m    205\u001b[0m loss_cls \u001b[38;5;241m=\u001b[39m criterion2(cls_out, target\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)) \n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# clustering loss\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m loss_clus \u001b[38;5;241m=\u001b[39m  \u001b[43mclusterloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m loss_clus \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([loss_clus])\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# AEloss\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 88\u001b[0m, in \u001b[0;36mclusterloss\u001b[1;34m(config, u, rho, reduction)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclusterloss\u001b[39m(config, u, rho \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 88\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_classes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     loss \u001b[38;5;241m=\u001b[39m silhouette_score(u\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach(),kmeans\u001b[38;5;241m.\u001b[39mlabels_)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1526\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1523\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;66;03m# run a k-means once\u001b[39;00m\n\u001b[1;32m-> 1526\u001b[0m labels, inertia, centers, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenters_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# determine if these results are the best so far\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# we chose a new run if it has a better inertia and the clustering is\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;66;03m# different from the best so far (it's possible that the inertia is\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;66;03m# slightly better even if the clustering is the same with potentially\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m \u001b[38;5;66;03m# permuted labels, due to rounding errors)\u001b[39;00m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_inertia \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1542\u001b[0m     inertia \u001b[38;5;241m<\u001b[39m best_inertia\n\u001b[0;32m   1543\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_same_clustering(labels, best_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters)\n\u001b[0;32m   1544\u001b[0m ):\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:688\u001b[0m, in \u001b[0;36m_kmeans_single_lloyd\u001b[1;34m(X, sample_weight, centers_init, max_iter, verbose, tol, n_threads)\u001b[0m\n\u001b[0;32m    684\u001b[0m strict_convergence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;66;03m# Threadpoolctl context to limit the number of threads in second level of\u001b[39;00m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;66;03m# nested parallelism (i.e. BLAS) to avoid oversubscription.\u001b[39;00m\n\u001b[1;32m--> 688\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mthreadpool_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[0;32m    690\u001b[0m         lloyd_iter(\n\u001b[0;32m    691\u001b[0m             X,\n\u001b[0;32m    692\u001b[0m             sample_weight,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    698\u001b[0m             n_threads,\n\u001b[0;32m    699\u001b[0m         )\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\sklearn\\utils\\fixes.py:72\u001b[0m, in \u001b[0;36mthreadpool_limits\u001b[1;34m(limits, user_api)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m controller\u001b[38;5;241m.\u001b[39mlimit(limits\u001b[38;5;241m=\u001b[39mlimits, user_api\u001b[38;5;241m=\u001b[39muser_api)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mthreadpoolctl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreadpool_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\threadpoolctl.py:171\u001b[0m, in \u001b[0;36mthreadpool_limits.__init__\u001b[1;34m(self, limits, user_api)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_api, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefixes \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(limits, user_api)\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_threadpool_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\threadpoolctl.py:268\u001b[0m, in \u001b[0;36mthreadpool_limits._set_threadpool_limits\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m modules \u001b[38;5;241m=\u001b[39m \u001b[43m_ThreadpoolInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m                          \u001b[49m\u001b[43muser_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_user_api\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# self._limits is a dict {key: num_threads} where key is either\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# a prefix or a user_api. If a module matches both, the limit\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# corresponding to the prefix is chosed.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mprefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits:\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\threadpoolctl.py:340\u001b[0m, in \u001b[0;36m_ThreadpoolInfo.__init__\u001b[1;34m(self, user_api, prefixes, modules)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m user_api \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m user_api\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_if_incompatible_openmp()\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\threadpoolctl.py:373\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._load_modules\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_dyld()\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 373\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_modules_with_enum_process_module_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_dl_iterate_phdr()\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\threadpoolctl.py:478\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._find_modules_with_enum_process_module_ex\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    474\u001b[0m n_size \u001b[38;5;241m=\u001b[39m DWORD()\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h_module \u001b[38;5;129;01min\u001b[39;00m h_modules:\n\u001b[0;32m    476\u001b[0m \n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# Get the path of the current module\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mps_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetModuleFileNameExW\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m            \u001b[49m\u001b[43mh_process\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetModuleFileNameEx failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    482\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m buf\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for INCV_c in config['INCV_C_list']:\n",
    "    #INCV_c = 0.9\n",
    "    if config['noise_pattern'] == 'asym':\n",
    "        INCV_b = 0.1\n",
    "    else:\n",
    "        INCV_b = INCV_c\n",
    "    \n",
    "    print(\"INCV_c:\",INCV_c)\n",
    "    DeepRe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879ed96a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
